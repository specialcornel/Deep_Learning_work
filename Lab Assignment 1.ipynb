{"cells":[{"cell_type":"markdown","metadata":{"id":"Fg8BSVK4OcFx"},"source":["# Lab Assignment 1"]},{"cell_type":"markdown","metadata":{"id":"Pit0BSlkOfcw"},"source":["Student name: Cornelius Adejoro"]},{"cell_type":"markdown","metadata":{"id":"NXp58l3vrC1L"},"source":["## Notebook version\n","\n","This notebook includes all the codes in the codebase of lab assignment 1. Completing and submitting this script is equivalent to submitting the codebase. Please note that your submitted script should include errorless cell outputs that contain necessary information that proves you have successfully run the notebook in your own directory.\n","\n","You can choose to (1) run this notebook locally on your end or (2) run this notebook on colab. For the former, you will need to download the dataset to your device that resembles the instructions for the codebase. For the latter, **you will need to upload the dataset to your Google Drive** account, and connect your colab notebook to your Google Drive. Then, go to \"File->Save a copy in Drive\" to create a copy you can edit.\n"]},{"cell_type":"markdown","metadata":{"id":"mWOk8c6QstJ2"},"source":["#### Colab (if applicable)\n","\n","If you are running this script on colab, uncomment and run the cell below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24118,"status":"ok","timestamp":1739945679112,"user":{"displayName":"Cornelius Adejoro","userId":"03709556582186281413"},"user_tz":420},"id":"OATj2nvHs2O1","outputId":"9e6a0d9a-8ff9-4b67-d52e-0c147b74c90c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"PHvOW4Qxs30Y"},"source":["Note that the Google Drive directory has the root `/content/drive/`. For instance, my directory to the dataset is `'/content/drive/My Drive/Courses/CSCI 5922/CSCI 5922 SP25/Demo/MNIST/'`."]},{"cell_type":"markdown","metadata":{"id":"2ufLsFPnq6gu"},"source":["### mnist.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqaf3LuXOa1c"},"outputs":[],"source":["#Original source: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n","#It has been modified for ease of use w/ pytorch\n","\n","#You do NOT need to modify ANY code in this file!\n","\n","import numpy as np\n","import struct\n","from array import array\n","import torch\n","\n","class MnistDataloader(object):\n","    def __init__(self, training_images_filepath,training_labels_filepath,\n","                 test_images_filepath, test_labels_filepath):\n","        self.training_images_filepath = training_images_filepath\n","        self.training_labels_filepath = training_labels_filepath\n","        self.test_images_filepath = test_images_filepath\n","        self.test_labels_filepath = test_labels_filepath\n","\n","    def read_images_labels(self, images_filepath, labels_filepath):\n","        n = 60000 if \"train\" in images_filepath else 10000\n","        labels = torch.zeros((n, 10))\n","        with open(labels_filepath, 'rb') as file:\n","            magic, size = struct.unpack(\">II\", file.read(8))\n","            if magic != 2049:\n","                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n","            l = torch.tensor(array(\"B\", file.read())).unsqueeze(-1)\n","            l = torch.concatenate((torch.arange(0, n).unsqueeze(-1), l), dim = 1).type(torch.int32)\n","            labels[l[:,0], l[:,1]] = 1\n","\n","        with open(images_filepath, 'rb') as file:\n","            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n","            if magic != 2051:\n","                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n","            image_data = array(\"B\", file.read())\n","        images = torch.zeros((n, 28**2))\n","        for i in range(size):\n","            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n","            #img = img.reshape(28, 28)\n","            images[i, :] = torch.tensor(img)\n","\n","        return images, labels\n","\n","    def load_data(self):\n","        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n","        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n","        return (x_train, y_train),(x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"EpKgf2fMquMh"},"source":["### activations.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuJUuwXrOoVg"},"outputs":[],"source":["import torch\n","\n","class ReLU():\n","  #Complete this class\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        return torch.maximum(x, torch.tensor(0.0))\n","\n","    def backward(self, delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n","        return delta * (x > 0).float()\n","\n","\n","class LeakyReLU():\n","  #Complete this class\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        return torch.maximum(0.1 * x, x)\n","\n","    def backward(self, delta: torch.tensor, x: torch.tensor) -> torch.tensor:\n","        return delta * torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.1))"]},{"cell_type":"markdown","metadata":{"id":"0L2zEHN7qxuh"},"source":["### framework.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51634,"status":"ok","timestamp":1739945736237,"user":{"displayName":"Cornelius Adejoro","userId":"03709556582186281413"},"user_tz":420},"id":"xjBDqIScO-hy","outputId":"2685908f-7b40-4be7-a647-c6aa9c2445fd"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 51.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.382489379654583\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 86.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 18.25%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 57.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.180727708034026\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 114.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 28.66%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 58.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.976637112788665\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 125.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 40.06%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 82.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.7913277403921144\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 137.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 48.99%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 77.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.631622124940921\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 147.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 54.85%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 78.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.4970486785611536\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 152.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 59.53%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 70.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.384220854848878\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 116.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 63.16%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 68.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.2892259787290523\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 142.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 65.95%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 75.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.2077473422400973\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 113.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 68.25%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 54.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.1381452002077022\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 98.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 70.24%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 51.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0776699154804914\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 113.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 72.05%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 65.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.024091926904825\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 142.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 73.47%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 79.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9772219005812947\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 150.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 74.52%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 79.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9356629761875185\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 157.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 75.92%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 77.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8981687253356999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 139.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 76.82%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 80.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8645242855080173\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 149.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 77.57%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 79.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8343855550146511\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 152.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.39%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 72.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8068519015597482\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 92.26it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.95%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 54.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7817512149484749\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 117.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 79.47%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 61.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7584617137908936\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 120.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.09%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 86.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7374428205001049\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 161.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.38%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 95.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7178384508842077\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 185.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.97%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 96.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6996176146034502\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 185.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.17%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 93.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6831291312845345\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 176.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.51%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6673054975322169\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 170.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.87%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import numpy as np\n","import tqdm\n","\n","class MLP:\n","    '''\n","    This class implements a generic MLP learning framework. The core structure is provided,\n","    but key functions such as initialize(), forward(), backward(), and TrainMLP() have been completed.\n","    '''\n","    def __init__(self, layer_sizes: list[int]):\n","        self.layer_sizes = layer_sizes\n","        self.num_layers = len(layer_sizes) - 1\n","        self.weights = []\n","        self.biases = []\n","        self.features = []\n","\n","        # Hyperparameters with default values\n","        self.learning_rate = 1e-6\n","        self.batch_size = 512\n","        self.epoch = 25\n","        self.activation_function = ReLU()\n","\n","    def set_hp(self, lr: float, bs: int, activation: object) -> None:\n","        self.learning_rate = lr\n","        self.batch_size = bs\n","        self.activation_function = activation\n","\n","    def initialize(self) -> None:\n","        # Initialize weights with uniform distribution +/- sqrt(6 / (d_in + d_out))\n","        for i in range(self.num_layers):\n","            d_in, d_out = self.layer_sizes[i], self.layer_sizes[i + 1]\n","            bound = np.sqrt(6 / (d_in + d_out))\n","            self.weights.append(torch.empty((d_in, d_out)).uniform_(-bound, bound))\n","            #self.weights.append(torch.rand((d_in, d_out)) * 2 * bound - bound)\n","            self.biases.append(torch.zeros(1, d_out))\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        # Forward propagation\n","        self.features = [x]\n","        for i in range(self.num_layers):\n","            x = x @ self.weights[i] + self.biases[i]\n","            x = self.activation_function.forward(x)\n","            self.features.append(x)\n","        logits = self.features[-1]\n","        softmax_x = torch.exp(logits) / torch.sum(torch.exp(logits), dim=1, keepdim=True)\n","        return softmax_x\n","\n","    def backward(self, delta: torch.tensor) -> None:\n","      for i in reversed(range(self.num_layers)):\n","          # Gradient of weights and biases\n","          grad_w = self.features[i].T @ delta\n","          grad_b = delta.mean(dim=0)\n","\n","          # Update weights and biases using gradient descent\n","          self.weights[i] -= self.learning_rate * grad_w\n","          self.biases[i] -= self.learning_rate * grad_b\n","\n","          # Compute delta for the previous layer (only if it's not the input layer)\n","          if i > 0:\n","              # grad_w = self.weights[i]\n","              # delta = delta @ grad_w.T\n","              # delta = self.activation_function.backward(delta, self.features[i - 1])\n","\n","              delta = self.activation_function.backward( (delta @ self.weights[i].T),(self.features[i]))\n","\n","              # grad_b = delta.mean(dim=0)\n","              # delta = (delta @ self.weights[i].T) * (self.features[i] > 0).float()  # Apply ReLU derivative\n","\n","def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n","    bs = model.batch_size\n","    N = x_train.shape[0]\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0  # Total loss\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n","\n","        # Forward pass\n","        y_hat = model.forward(x)\n","\n","        # Compute cross-entropy loss (convert one-hot to class indices)\n","        target = torch.argmax(y, dim=1)\n","        l = -torch.sum(y * torch.log(y_hat))\n","        L += l.item()\n","\n","        # Backward pass\n","        delta = y_hat - y  # Gradient for softmax + cross-entropy\n","        model.backward(delta)\n","\n","    print(\"Train Loss:\", L / ((N // bs) * bs))\n","\n","def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n","    bs = model.batch_size\n","    N = x_test.shape[0]\n","\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0\n","    A = 0\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n","\n","        y_hat = model.forward(x)\n","        target = torch.argmax(y, dim=1)\n","\n","        # Cross-entropy loss\n","        l = torch.nn.functional.cross_entropy(y_hat, target)\n","        L += l.item()\n","\n","        # Accuracy calculation\n","        predictions = torch.argmax(y_hat, dim=1)\n","        A += torch.sum(predictions == target).item()\n","\n","    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / N))\n","\n","def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n","    base_path = \"/content/drive/MyDrive/Neural_Networks/Data/\"\n","\n","    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n","                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n","    x_std = torch.std(x_train, dim=0, keepdim=True)\n","\n","    x_train -= x_mean\n","    x_train /= x_std\n","    x_train[x_train != x_train] = 0\n","\n","    x_test -= x_mean\n","    x_test /= x_std\n","    x_test[x_test != x_test] = 0\n","\n","    return x_train, y_train, x_test, y_test\n","\n","def main():\n","    x_train, y_train, x_test, y_test = normalize_mnist()\n","\n","    model = MLP([784, 256, 10])\n","    model.initialize()\n","    model.set_hp(lr=1e-6, bs=512, activation=ReLU())\n","\n","    E = 25\n","    for _ in range(E):\n","        TrainMLP(model, x_train, y_train)\n","        TestMLP(model, x_test, y_test)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{"id":"vnHqphvTu0Rc"},"source":["Part Two"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12236,"status":"ok","timestamp":1739945748470,"user":{"displayName":"Cornelius Adejoro","userId":"03709556582186281413"},"user_tz":420},"id":"WjDW4JZNu4Lf","outputId":"a329294e-d49c-47ab-ae7f-64ee1545692e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 435.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.302205695046319\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 506.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 16.06%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 434.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2184677674220157\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 530.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 20.79%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 303.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.143950425661527\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 402.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 25.19%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 312.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.0760872251967077\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 449.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 29.85%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 293.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.0141556782600207\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 311.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 34.05%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 325.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.9571993473248603\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 445.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 38.25%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 320.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.9040688378179176\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 439.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 41.50%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 309.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.8544650240841074\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 478.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 44.00%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 307.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.8075149405715811\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 362.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 46.16%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 289.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.7632070851122212\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 391.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 47.92%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 297.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.7210400461131692\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 405.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 49.62%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 284.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.6810490320890377\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 464.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 51.03%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 288.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.643571894393008\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 486.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 52.56%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 416.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.6075060184185321\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 478.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 53.51%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 406.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.5732674333784316\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 406.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 54.64%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 421.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.5410104327731662\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 565.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 55.85%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 406.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.509583436525785\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 423.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 56.79%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 420.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.4797548045459976\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 538.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 57.66%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 405.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.451578830042456\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 496.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 58.64%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 396.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.4243504399927254\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 475.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 59.49%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 412.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.398268432698698\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 617.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 60.21%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 410.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.3727119111607218\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 521.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 60.93%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 394.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.3486182047770574\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 468.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 61.62%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 406.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.3252385976987007\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 464.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 62.23%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:00<00:00, 408.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.3024882017037807\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 547.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 62.83%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import numpy as np\n","import tqdm\n","\n","class MLP:\n","    '''\n","    This class implements a generic MLP learning framework. The core structure is provided,\n","    but key functions such as initialize(), forward(), backward(), and TrainMLP() have been completed.\n","    '''\n","    def __init__(self, layer_sizes: list[int]):\n","        self.layer_sizes = layer_sizes\n","        self.num_layers = len(layer_sizes) - 1\n","        self.weights = []\n","        self.biases = []\n","        self.features = []\n","\n","        # Hyperparameters with default values\n","        self.learning_rate = 1e-6\n","        self.batch_size = 512\n","        self.epoch = 25\n","        self.activation_function = ReLU()\n","\n","    def set_hp(self, lr: float, bs: int, activation: object) -> None:\n","        self.learning_rate = lr\n","        self.batch_size = bs\n","        self.activation_function = activation\n","\n","    def initialize(self) -> None:\n","        # Initialize weights with uniform distribution +/- sqrt(6 / (d_in + d_out))\n","        for i in range(self.num_layers):\n","            d_in, d_out = self.layer_sizes[i], self.layer_sizes[i + 1]\n","            bound = np.sqrt(6 / (d_in + d_out))\n","            self.weights.append(torch.empty((d_in, d_out)).uniform_(-bound, bound))\n","            #self.weights.append(torch.rand((d_in, d_out)) * 2 * bound - bound)\n","            self.biases.append(torch.zeros(1, d_out))\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        # Forward propagation\n","        self.features = [x]\n","        for i in range(self.num_layers):\n","            x = x @ self.weights[i] + self.biases[i]\n","            x = self.activation_function.forward(x)\n","            self.features.append(x)\n","        logits = self.features[-1]\n","        softmax_x = torch.exp(logits) / torch.sum(torch.exp(logits), dim=1, keepdim=True)\n","        return softmax_x\n","\n","    def backward(self, delta: torch.tensor) -> None:\n","      for i in reversed(range(self.num_layers)):\n","          # Gradient of weights and biases\n","          grad_w = self.features[i].T @ delta\n","          grad_b = delta.mean(dim=0)\n","\n","          # Update weights and biases using gradient descent\n","          self.weights[i] -= self.learning_rate * grad_w\n","          self.biases[i] -= self.learning_rate * grad_b\n","\n","          # Compute delta for the previous layer (only if it's not the input layer)\n","          if i > 0:\n","              # grad_w = self.weights[i]\n","              # delta = delta @ grad_w.T\n","              # delta = self.activation_function.backward(delta, self.features[i - 1])\n","\n","              delta = self.activation_function.backward( (delta @ self.weights[i].T),(self.features[i]))\n","\n","              # grad_b = delta.mean(dim=0)\n","              # delta = (delta @ self.weights[i].T) * (self.features[i] > 0).float()  # Apply ReLU derivative\n","\n","def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n","    bs = model.batch_size\n","    N = x_train.shape[0]\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0  # Total loss\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n","\n","        # Forward pass\n","        y_hat = model.forward(x)\n","\n","        # Compute cross-entropy loss (convert one-hot to class indices)\n","        target = torch.argmax(y, dim=1)\n","        l = -torch.sum(y * torch.log(y_hat))\n","        L += l.item()\n","\n","        # Backward pass\n","        delta = y_hat - y  # Gradient for softmax + cross-entropy\n","        model.backward(delta)\n","\n","    print(\"Train Loss:\", L / ((N // bs) * bs))\n","\n","def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n","    bs = model.batch_size\n","    N = x_test.shape[0]\n","\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0\n","    A = 0\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n","\n","        y_hat = model.forward(x)\n","        target = torch.argmax(y, dim=1)\n","\n","        # Cross-entropy loss\n","        l = torch.nn.functional.cross_entropy(y_hat, target)\n","        L += l.item()\n","\n","        # Accuracy calculation\n","        predictions = torch.argmax(y_hat, dim=1)\n","        A += torch.sum(predictions == target).item()\n","\n","    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / N))\n","\n","def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n","    base_path = \"/content/drive/MyDrive/Neural_Networks/Data/\"\n","\n","    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n","                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n","    x_std = torch.std(x_train, dim=0, keepdim=True)\n","\n","    x_train -= x_mean\n","    x_train /= x_std\n","    x_train[x_train != x_train] = 0\n","\n","    x_test -= x_mean\n","    x_test /= x_std\n","    x_test[x_test != x_test] = 0\n","\n","    return x_train, y_train, x_test, y_test\n","\n","def main():\n","    x_train, y_train, x_test, y_test = normalize_mnist()\n","\n","    model = MLP([784, 10, 10])\n","    model.initialize()\n","    model.set_hp(lr=1e-6, bs=512, activation=ReLU())\n","\n","    E = 25\n","    for _ in range(E):\n","        TrainMLP(model, x_train, y_train)\n","        TestMLP(model, x_test, y_test)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{"id":"HDs2KKaEClGc"},"source":["# L1 Normaliation Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77YOFQddC35D"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CA3sthtOCq37","executionInfo":{"status":"ok","timestamp":1739945791547,"user_tz":420,"elapsed":43079,"user":{"displayName":"Cornelius Adejoro","userId":"03709556582186281413"}},"outputId":"0c62384b-c804-44bd-f476-aca1ac07a09c"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4150270885891385\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 165.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 17.93%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.1778263903071737\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 182.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 31.49%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 73.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.9581532111534705\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 119.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 44.32%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 57.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.767588411640917\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 103.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 53.74%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 66.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.6076688623835897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 167.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 60.07%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.475569242086166\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 181.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 64.28%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.365491582797124\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 181.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 67.34%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.2732429718359923\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 184.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 69.45%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.194974283886771\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 182.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 71.26%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.1278455950256088\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 166.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 72.74%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0694005051229754\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 165.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 74.09%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 80.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0182970879424331\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 66.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 75.36%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 51.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9732567326635377\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 118.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 76.08%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 63.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9326199588612614\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 176.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.003720993629509681 Test Accuracy: 77.03%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8965492146646875\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 160.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 77.58%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8639485841123467\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 186.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.31%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8346190773523771\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 183.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.72%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 87.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.807703585196764\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 192.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 79.41%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7832118904488719\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 172.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 79.83%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7607280515198015\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 161.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.41%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 86.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7401192870914427\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 116.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.85%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 59.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7209877494053963\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 122.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.29%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 58.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.703241520967239\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 119.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.78%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 86.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6867105767258213\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 183.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.89%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.671353425225641\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 180.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 82.08%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import numpy as np\n","import tqdm\n","\n","class MLP:\n","    '''\n","    This class implements a generic MLP learning framework. The core structure is provided,\n","    but key functions such as initialize(), forward(), backward(), and TrainMLP() have been completed.\n","    '''\n","    def __init__(self, layer_sizes: list[int]):\n","        self.layer_sizes = layer_sizes\n","        self.num_layers = len(layer_sizes) - 1\n","        self.weights = []\n","        self.biases = []\n","        self.features = []\n","\n","        # Hyperparameters with default values\n","        self.learning_rate = 1e-6\n","        self.batch_size = 512\n","        self.epoch = 25\n","        self.activation_function = ReLU()\n","\n","    def set_hp(self, lr: float, bs: int, activation: object, l1_reg: float = 0.0) -> None: # Added l1_reg with a default value\n","        self.learning_rate = lr\n","        self.batch_size = bs\n","        self.activation_function = activation\n","        self.l1_reg = l1_reg # Assign l1_reg to the model\n","\n","    def initialize(self) -> None:\n","        # Initialize weights with uniform distribution +/- sqrt(6 / (d_in + d_out))\n","        for i in range(self.num_layers):\n","            d_in, d_out = self.layer_sizes[i], self.layer_sizes[i + 1]\n","            bound = np.sqrt(6 / (d_in + d_out))\n","            self.weights.append(torch.empty((d_in, d_out)).uniform_(-bound, bound))\n","            #self.weights.append(torch.rand((d_in, d_out)) * 2 * bound - bound)\n","            self.biases.append(torch.zeros(1, d_out))\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        # Forward propagation\n","        self.features = [x]\n","        for i in range(self.num_layers):\n","            x = x @ self.weights[i] + self.biases[i]\n","            x = self.activation_function.forward(x)\n","            self.features.append(x)\n","        logits = self.features[-1]\n","        softmax_x = torch.exp(logits) / torch.sum(torch.exp(logits), dim=1, keepdim=True)\n","        return softmax_x\n","\n","    def backward(self, delta: torch.tensor) -> None:\n","      for i in reversed(range(self.num_layers)):\n","          # Gradient of weights and biases\n","          grad_w = self.features[i].T @ delta\n","          grad_b = delta.mean(dim=0)\n","\n","          # Update weights and biases using gradient descent\n","          self.weights[i] -= self.learning_rate * (grad_w + self.l1_reg * torch.sign(self.weights[i]))\n","          self.biases[i] -= self.learning_rate * grad_b\n","\n","          # Compute delta for the previous layer (only if it's not the input layer)\n","          if i > 0:\n","              # grad_w = self.weights[i]\n","              # delta = delta @ grad_w.T\n","              # delta = self.activation_function.backward(delta, self.features[i - 1])\n","\n","              delta = self.activation_function.backward( (delta @ self.weights[i].T),(self.features[i]))\n","\n","              # grad_b = delta.mean(dim=0)\n","              # delta = (delta @ self.weights[i].T) * (self.features[i] > 0).float()  # Apply ReLU derivative\n","\n","def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n","    bs = model.batch_size\n","    N = x_train.shape[0]\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0  # Total loss\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n","\n","        # Forward pass\n","        y_hat = model.forward(x)\n","\n","        # Compute cross-entropy loss (convert one-hot to class indices)\n","        target = torch.argmax(y, dim=1)\n","        l = -torch.sum(y * torch.log(y_hat))\n","        L += l.item()\n","\n","        # Backward pass\n","        delta = y_hat - y  # Gradient for softmax + cross-entropy\n","        model.backward(delta)\n","\n","    print(\"Train Loss:\", L / ((N // bs) * bs))\n","\n","def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n","    bs = model.batch_size\n","    N = x_test.shape[0]\n","\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0\n","    A = 0\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n","\n","        y_hat = model.forward(x)\n","        target = torch.argmax(y, dim=1)\n","\n","        # Cross-entropy loss\n","        l = torch.nn.functional.cross_entropy(y_hat, target)\n","        L += l.item()\n","\n","        # Accuracy calculation\n","        predictions = torch.argmax(y_hat, dim=1)\n","        A += torch.sum(predictions == target).item()\n","\n","    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / N))\n","\n","def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n","    base_path = \"/content/drive/MyDrive/Neural_Networks/Data/\"\n","\n","    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n","                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n","    x_std = torch.std(x_train, dim=0, keepdim=True)\n","\n","    x_train -= x_mean\n","    x_train /= x_std\n","    x_train[x_train != x_train] = 0\n","\n","    x_test -= x_mean\n","    x_test /= x_std\n","    x_test[x_test != x_test] = 0\n","\n","    return x_train, y_train, x_test, y_test\n","\n","def main():\n","    x_train, y_train, x_test, y_test = normalize_mnist()\n","\n","    model = MLP([784, 256, 10])\n","    model.initialize()\n","    model.set_hp(lr=1e-6, bs=512, activation=ReLU(), l1_reg=0.01) # Pass l1_reg as an argument\n","\n","    E = 25\n","    for _ in range(E):\n","        TrainMLP(model, x_train, y_train)\n","        TestMLP(model, x_test, y_test)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]},{"cell_type":"markdown","source":["# L2 Regulization"],"metadata":{"id":"KkiohUCRzDCw"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import tqdm\n","\n","class MLP:\n","    '''\n","    This class implements a generic MLP learning framework. The core structure is provided,\n","    but key functions such as initialize(), forward(), backward(), and TrainMLP() have been completed.\n","    '''\n","    def __init__(self, layer_sizes: list[int]):\n","        self.layer_sizes = layer_sizes\n","        self.num_layers = len(layer_sizes) - 1\n","        self.weights = []\n","        self.biases = []\n","        self.features = []\n","\n","        # Hyperparameters with default values\n","        self.learning_rate = 1e-6\n","        self.batch_size = 512\n","        self.epoch = 25\n","        self.activation_function = ReLU()\n","\n","    def set_hp(self, lr: float, bs: int, activation: object, l2_reg: float = 0.0) -> None: # Added l2_reg with a default value\n","        self.learning_rate = lr\n","        self.batch_size = bs\n","        self.activation_function = activation\n","        self.l2_reg = l2_reg # Assign l2_reg to the model\n","\n","    def initialize(self) -> None:\n","        # Initialize weights with uniform distribution +/- sqrt(6 / (d_in + d_out))\n","        for i in range(self.num_layers):\n","            d_in, d_out = self.layer_sizes[i], self.layer_sizes[i + 1]\n","            bound = np.sqrt(6 / (d_in + d_out))\n","            self.weights.append(torch.empty((d_in, d_out)).uniform_(-bound, bound))\n","            #self.weights.append(torch.rand((d_in, d_out)) * 2 * bound - bound)\n","            self.biases.append(torch.zeros(1, d_out))\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        # Forward propagation\n","        self.features = [x]\n","        for i in range(self.num_layers):\n","            x = x @ self.weights[i] + self.biases[i]\n","            x = self.activation_function.forward(x)\n","            self.features.append(x)\n","        logits = self.features[-1]\n","        softmax_x = torch.exp(logits) / torch.sum(torch.exp(logits), dim=1, keepdim=True)\n","        return softmax_x\n","\n","    def backward(self, delta: torch.tensor) -> None:\n","      for i in reversed(range(self.num_layers)):\n","          # Gradient of weights and biases\n","          grad_w = self.features[i].T @ delta\n","          grad_b = delta.mean(dim=0)\n","\n","          # Update weights and biases using gradient descent\n","          self.weights[i] -= self.learning_rate * (grad_w + self.l2_reg * torch.sign(self.weights[i]))\n","          self.biases[i] -= self.learning_rate * grad_b\n","\n","          # Compute delta for the previous layer (only if it's not the input layer)\n","          if i > 0:\n","              # grad_w = self.weights[i]\n","              # delta = delta @ grad_w.T\n","              # delta = self.activation_function.backward(delta, self.features[i - 1])\n","\n","              delta = self.activation_function.backward( (delta @ self.weights[i].T),(self.features[i]))\n","\n","              # grad_b = delta.mean(dim=0)\n","              # delta = (delta @ self.weights[i].T) * (self.features[i] > 0).float()  # Apply ReLU derivative\n","\n","def TrainMLP(model: MLP, x_train: torch.tensor, y_train: torch.tensor) -> MLP:\n","    bs = model.batch_size\n","    N = x_train.shape[0]\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0  # Total loss\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_train[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_train[idx[i * bs:(i + 1) * bs], ...]\n","\n","        # Forward pass\n","        y_hat = model.forward(x)\n","\n","        # Compute cross-entropy loss (convert one-hot to class indices)\n","        target = torch.argmax(y, dim=1)\n","        l = -torch.sum(y * torch.log(y_hat))\n","        L += l.item()\n","\n","        # Backward pass\n","        delta = y_hat - y  # Gradient for softmax + cross-entropy\n","        model.backward(delta)\n","\n","    print(\"Train Loss:\", L / ((N // bs) * bs))\n","\n","def TestMLP(model: MLP, x_test: torch.tensor, y_test: torch.tensor) -> tuple[float, float]:\n","    bs = model.batch_size\n","    N = x_test.shape[0]\n","\n","    rng = np.random.default_rng()\n","    idx = rng.permutation(N)\n","\n","    L = 0\n","    A = 0\n","\n","    for i in tqdm.tqdm(range(N // bs)):\n","        x = x_test[idx[i * bs:(i + 1) * bs], ...]\n","        y = y_test[idx[i * bs:(i + 1) * bs], ...]\n","\n","        y_hat = model.forward(x)\n","        target = torch.argmax(y, dim=1)\n","\n","        # Cross-entropy loss\n","        l = torch.nn.functional.cross_entropy(y_hat, target)\n","        L += l.item()\n","\n","        # Accuracy calculation\n","        predictions = torch.argmax(y_hat, dim=1)\n","        A += torch.sum(predictions == target).item()\n","\n","    print(\"Test Loss:\", L / ((N // bs) * bs), \"Test Accuracy: {:.2f}%\".format(100 * A / N))\n","\n","def normalize_mnist() -> tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n","    base_path = \"/content/drive/MyDrive/Neural_Networks/Data/\"\n","\n","    mnist = MnistDataloader(base_path + \"train-images.idx3-ubyte\", base_path + \"train-labels.idx1-ubyte\",\n","                            base_path + \"t10k-images.idx3-ubyte\", base_path + \"t10k-labels.idx1-ubyte\")\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","    x_mean = torch.mean(x_train, dim=0, keepdim=True)\n","    x_std = torch.std(x_train, dim=0, keepdim=True)\n","\n","    x_train -= x_mean\n","    x_train /= x_std\n","    x_train[x_train != x_train] = 0\n","\n","    x_test -= x_mean\n","    x_test /= x_std\n","    x_test[x_test != x_test] = 0\n","\n","    return x_train, y_train, x_test, y_test\n","\n","def main():\n","    x_train, y_train, x_test, y_test = normalize_mnist()\n","\n","    model = MLP([784, 256, 10])\n","    model.initialize()\n","    model.set_hp(lr=1e-6, bs=512, activation=ReLU(), l2_reg=0.01) # Pass l2_reg as an argument\n","\n","    E = 25\n","    for _ in range(E):\n","        TrainMLP(model, x_train, y_train)\n","        TestMLP(model, x_test, y_test)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0DZdzprzJ_n","executionInfo":{"status":"ok","timestamp":1739945834861,"user_tz":420,"elapsed":43317,"user":{"displayName":"Cornelius Adejoro","userId":"03709556582186281413"}},"outputId":"0f281d81-d9bc-44b7-d60a-d7693986d552"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 93.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3131020904606223\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 181.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 23.64%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 88.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.1006237604679208\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 164.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 35.78%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 92.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.892641266187032\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 186.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 46.48%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 70.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.710998530061836\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 120.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 54.15%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 58.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.5596820309630826\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 122.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 59.85%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 67.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.4342170372987404\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 162.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.004044992138484591 Test Accuracy: 63.76%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.3288457699311085\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 182.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 67.00%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.2392904452788525\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 179.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 69.43%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.1627229523454976\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 161.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 71.71%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 87.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0965666669046776\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 166.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 73.42%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0386668148203793\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 172.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 74.68%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 87.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9884375690394996\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 164.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 75.73%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 78.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9432454791843382\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 118.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 76.78%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 57.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.9039876410084912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 103.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 77.55%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 64.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8686666722990509\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 187.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.21%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8368792472741543\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 190.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 78.91%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 92.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.8082772931482038\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 184.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 79.47%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 89.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.782122876909044\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 160.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.01%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7584307051112509\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 184.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.32%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 91.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7371624636853862\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 183.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 80.88%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 88.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7169902523358663\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 188.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.003540409973969585 Test Accuracy: 81.34%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:02<00:00, 56.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6988479586747977\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 115.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.70%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 58.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.682037250099019\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 117.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 81.98%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 64.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6662619990161341\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 183.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 82.24%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 117/117 [00:01<00:00, 90.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6516790858700744\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 180.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: nan Test Accuracy: 82.35%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1PsbWzkZkbgktdgoWm_Aa2-lCsSplDWPw","timestamp":1739245578821}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}